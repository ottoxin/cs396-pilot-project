run_name: strong
base_model: Qwen/Qwen2.5-1.5B-Instruct
seed: 42
train_data:
  source: file
  path: data/gsm8k_train_self-instruct.jsonl
  split: train
method: qlora
train_hparams:
  num_epochs: 2
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.1
max_seq_length: 2048
fewshot_k: 8
max_new_tokens_gsm8k: 1024
max_new_tokens_ailuminate: 512
quantization:
  use_bnb: true
  double_quant: true
  nf4: true
  compute_dtype: bfloat16
checkpointing:
  save_steps: 400
  eval_steps: 400
  checkpoint_sweep: false
smoke_test:
  enabled: false
  train_samples: 200
  gsm8k_eval_samples: 50
  ailuminate_eval_samples: 50
data_paths:
  fewshot_file: data/fewshot_gsm8k_fixed.jsonl
  refined_gsm8k: data/gsm8k_train_self-instruct.jsonl
  ailuminate: data/ailuminate.jsonl
  safety_prompt: data/safety_prompt.txt
  safeguard_model: data/safeguard_model
output_dir: results
